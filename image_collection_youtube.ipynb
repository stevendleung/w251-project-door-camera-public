{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "from pytube import YouTube\n",
    "import cv2\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#where to save \n",
    "SAVE_PATH = 'youtube_scraping'\n",
    "  \n",
    "#link of the video to be downloaded \n",
    "link=[\n",
    "    ]\n",
    "  \n",
    "for i in link: \n",
    "    try: \n",
    "          \n",
    "        # object creation using YouTube\n",
    "        # which was imported in the beginning \n",
    "        yt = YouTube(i) \n",
    "    except: \n",
    "          \n",
    "        #to handle exception \n",
    "        print(\"Connection Error\") \n",
    "      \n",
    "    #filters out all the files with \"mp4\" extension \n",
    "    mp4files = yt.filter('mp4') \n",
    "  \n",
    "    # get the video with the extension and\n",
    "    # resolution passed in the get() function \n",
    "    d_video = yt.get(mp4files[-1].extension,mp4files[-1].resolution) \n",
    "    try: \n",
    "        # downloading the video \n",
    "        d_video.download(SAVE_PATH) \n",
    "    except: \n",
    "        print(\"Some Error!\") \n",
    "print('Task Completed!') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated from Search Query- needs work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(text, limit=10):\n",
    "    '''Return list of youtube urls based on search query'''\n",
    "    query = urllib.parse.quote(text)\n",
    "    url = \"https://www.youtube.com/results?search_query=\" + query\n",
    "    response = urllib.request.urlopen(url)\n",
    "    html = response.read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    urls = []\n",
    "    for i, vid in enumerate(soup.findAll(attrs={'class':'yt-uix-tile-link'})):\n",
    "        if i < limit:\n",
    "            urls.append('https://www.youtube.com' + vid['href'])\n",
    "    print(f\"Found {len(urls)} video links for {text}\")\n",
    "    return urls\n",
    "\n",
    "def download_video(url, path=None, max_duration=10):\n",
    "    '''download videos based on urls through pytube'''\n",
    "      try:\n",
    "        yt = YouTube(url)\n",
    "        duration = int(yt.player_config_args['player_response']['streamingData']['formats'][0]['approxDurationMs'])\n",
    "        if duration < max_duration*60*1000:\n",
    "            yt = yt.streams.filter(file_extension='mp4').first()\n",
    "            out_file = yt.download(path)\n",
    "            file_name = out_file.split(\"\\\\\")[-1]\n",
    "            print(f\"Downloaded {file_name} correctly!\")\n",
    "        else:\n",
    "            print(f\"Video {url} too long\")\n",
    "      except Exception as exc:\n",
    "        print(f\"Download of {url} did not work because of {exc}...\")\n",
    "        \n",
    "def extract_images_from_video(video, folder=None, delay=30, name=\"file\", max_images=20, silent=False):    \n",
    "    '''use opencv to extract images from video file'''\n",
    "    vidcap = cv2.VideoCapture(video)\n",
    "    count = 0\n",
    "    num_images = 0\n",
    "    if not folder:\n",
    "        folder = os.getcwd()\n",
    "    label = max_label(name, folder)\n",
    "    success = True\n",
    "    fps = int(vidcap.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    while success and num_images < max_images:\n",
    "        success, image = vidcap.read()\n",
    "        num_images += 1\n",
    "        label += 1\n",
    "        file_name = name + \"_\" + str(label) + \".jpg\"\n",
    "        path = os.path.join(folder, file_name)\n",
    "        cv2.imwrite(path, image)\n",
    "        if cv2.imread(path) is None:\n",
    "            os.remove(path)\n",
    "        else:\n",
    "            if not silent:\n",
    "                print(f'Image successfully written at {path}')\n",
    "        count += delay*fps\n",
    "        vidcap.set(1, count)\n",
    "        \n",
    "def extract_images_from_word(text, delete_video=False, image_delay=30, \n",
    "                             num_urls=10, max_images=20, name=\"youtube_scraping\", max_duration=15, silent=False):\n",
    "    '''main function to extract images from youtube videos based on search query'''\n",
    "    \n",
    "    if not os.path.exists(name):\n",
    "        os.mkdir(name)\n",
    "    urls = get_urls(text, num_urls)\n",
    "    for url in urls:\n",
    "        download_video(url, max_duration=max_duration)\n",
    "    for i, video in enumerate(glob.glob(\"*.mp4\")):\n",
    "        extract_images_from_video(video, folder=name, delay=image_delay, name=name, max_images=max_images, silent=silent)\n",
    "        if delete_video:\n",
    "            os.remove(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
